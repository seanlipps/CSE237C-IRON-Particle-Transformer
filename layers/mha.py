import numpy as np
from typing import List, Optional
from .base import AIELayer
from utils.tiling import tile_matrix
from utils.np_mha_linear import NumpyMHALinear


class MHALayer(AIELayer):

    def __init__(
        self,
        name: str,
        Wq: np.ndarray,
        Wk: np.ndarray,
        Wv: np.ndarray,
        Wo: np.ndarray,
        num_heads: int,
        d_model: int = 64,
        T: int = 160
    ):
        """
        Initialize MHA layer.

        Args:
            name: Layer name
            Wq: Query weight matrix (d_model, d_model), int8
            Wk: Key weight matrix (d_model, d_model), int8
            Wv: Value weight matrix (d_model, d_model), int8
            Wo: Output weight matrix (d_model, d_model), int8
            num_heads: Number of attention heads (1 or 4)
            d_model: Model dimension
            T: Sequence length (padded)

        Note: Tiling parameters (m, k, n) are set by AIEModel when layer is added.
        """
        super().__init__(name, 'mha', params={
            'Wq': Wq,
            'Wk': Wk,
            'Wv': Wv,
            'Wo': Wo,
            'num_heads': num_heads,
            'd_model': d_model,
            'T': T
        })

        self.Wq = Wq
        self.Wk = Wk
        self.Wv = Wv
        self.Wo = Wo
        self.num_heads = num_heads
        self.d_model = d_model
        self.T = T
        self.head_dim = d_model // num_heads

        self.m = None
        self.k = None
        self.n = None

        assert d_model % num_heads == 0, f"d_model {d_model} must be divisible by num_heads {num_heads}"
        assert num_heads in [1, 4], f"Only num_heads=1 or 4 supported, got {num_heads}"

        self.layer_q = None
        self.layer_k = None
        self.layer_v = None
        self.layer_o = None
        self.shift_q = None
        self.shift_k = None
        self.shift_v = None
        self.shift_s = None
        self.shift_c = None
        self.shift_o = None
        self.Wq_heads = []
        self.Wk_heads = []
        self.Wv_heads = []
        self.Wo_tiled = None

    def _compute_golden(self, inputs: List[np.ndarray]) -> np.ndarray:
        self.validate_inputs(inputs, expected_count=1)
        x = inputs[0]

        assert self.m is not None and self.k is not None and self.n is not None, \
            f"Tiling parameters not set. Layer must be added to AIEModel first."

        assert x.shape == (self.T, self.d_model), \
            f"Expected input shape ({self.T}, {self.d_model}), got {x.shape}"

        layers_list = []
        mha = NumpyMHALinear(
            d_model=self.d_model,
            num_heads=self.num_heads,
            name_prefix=self.name,
            Wq=self.Wq,
            Wk=self.Wk,
            Wv=self.Wv,
            Wo=self.Wo
        )

        output = mha(x, x, x, layers=layers_list)

        assert len(layers_list) == 4, f"Expected 4 layers from MHA, got {len(layers_list)}"
        self.layer_q = layers_list[0]
        self.layer_k = layers_list[1]
        self.layer_v = layers_list[2]
        self.layer_o = layers_list[3]
        self.shift_q = self.layer_q['shift']
        self.shift_k = self.layer_k['shift']
        self.shift_v = self.layer_v['shift']
        self.shift_s = self.layer_q['shift_scores']  # Per-head list
        self.shift_c = self.layer_q['shift_context']  # Per-head list
        self.shift_o = self.layer_o['shift']

        # Split and tile weight matrices per head
        for h in range(self.num_heads):
            col_start = h * self.head_dim
            col_end = (h + 1) * self.head_dim

            Wq_h = self.Wq[:, col_start:col_end]
            Wk_h = self.Wk[:, col_start:col_end]
            Wv_h = self.Wv[:, col_start:col_end]

            Wq_h_tiled = tile_matrix(Wq_h, self.k, self.n)
            Wk_h_tiled = tile_matrix(Wk_h, self.k, self.n)
            Wv_h_tiled = tile_matrix(Wv_h, self.k, self.n)

            self.Wq_heads.append(Wq_h_tiled)
            self.Wk_heads.append(Wk_h_tiled)
            self.Wv_heads.append(Wv_h_tiled)

        self.Wo_tiled = tile_matrix(self.Wo, self.k, self.n)

        self.outputs['a'] = output
        self._golden_computed = True

        return output

    def generate_kernel_code(self, f) -> None:
        assert self._golden_computed, "Must call compute_golden() before generating code"

        try:
            f.write(f"// MHA layer {self.idx}: uses per-head/source compilation units (layer_{self.idx}_*.cc).\n")
        except Exception:
            pass

        for h in range(self.num_heads):
            print("self.num_heads\n")
            with open(f"iron_kernels/layer_{self.idx}_q_head{h}.cc", "w") as fq:
                fq.write('#include "iron_kernels.h"\n\n')
                
                fq.write('extern "C" {\n')
                fq.write('#include <cstdint>\n')
                fq.write(f'int8_t k_p [{self.Wq_heads[h].size}] = {{ ')
                fq.write(', '.join(str(int(x)) for x in self.Wq_heads[h]))
                fq.write(' };\n\n')
                
                fq.write(f'void q{self.idx}_head{h}(int8_t * x, int8_t * a){{\n')
                fq.write(f'    dense<{self.m}, {self.k}, {self.n}, {self.T//self.m}, {self.d_model//self.k}, {self.head_dim//self.n}, {self.shift_q}, false>')
                fq.write('(x, a, k_p);\n')
                fq.write('}\n')
                fq.write('}')

            with open(f"iron_kernels/layer_{self.idx}_k_head{h}.cc", "w") as fk:
                fk.write('#include "iron_kernels.h"\n\n')
                
                fk.write('extern "C" {\n')
                fk.write('#include <cstdint>\n')
                fk.write(f'int8_t k_p [{self.Wk_heads[h].size}] = {{ ')
                fk.write(', '.join(str(int(x)) for x in self.Wk_heads[h]))
                fk.write(' };\n\n')
                
                fk.write(f'void k{self.idx}_head{h}(int8_t * x, int8_t * a){{\n')
                fk.write(f'    dense<{self.m}, {self.k}, {self.n}, {self.T//self.m}, {self.d_model//self.k}, {self.head_dim//self.n}, {self.shift_k}, false>')
                fk.write('(x, a, k_p);\n')
                fk.write('}\n')
                fk.write('}')

            with open(f"iron_kernels/layer_{self.idx}_v_head{h}.cc", "w") as fv:
                fv.write('#include "iron_kernels.h"\n\n')
                
                fv.write('extern "C" {\n')
                fv.write('#include <cstdint>\n')
                fv.write(f'int8_t k_p [{self.Wv_heads[h].size}] = {{ ')
                fv.write(', '.join(str(int(x)) for x in self.Wv_heads[h]))
                fv.write(' };\n\n')
                
                fv.write(f'void v{self.idx}_head{h}(int8_t * x, int8_t * a){{\n')
                fv.write(f'dense<{self.m}, {self.k}, {self.n}, {self.T//self.m}, {self.d_model//self.k}, {self.head_dim//self.n}, {self.shift_v}, false>')
                fv.write('(x, a, k_p);\n')
                fv.write('}\n')
                fv.write('}')

            with open(f"iron_kernels/layer_{self.idx}_scores_head{h}.cc", "w") as fs:
                fs.write('#include "iron_kernels.h"\n\n')
                fs.write('extern "C" {\n')
                fs.write(f'void scores{self.idx}_head{h}(int8_t * q_head, int8_t * k_head, int8_t * o_head){{\n')
                fs.write(f'    scores<{self.m}, {self.k}, {self.n}, {self.T//self.m}, {self.head_dim//self.k}, {self.head_dim//self.n}, {self.head_dim}, {self.T}, {self.shift_s[h]}>')
                fs.write('(q_head, k_head, o_head);\n')
                fs.write('}\n')
                fs.write('}')

            with open(f"iron_kernels/layer_{self.idx}_context_head{h}.cc", "w") as fc:
                fc.write('#include "iron_kernels.h"\n\n')

                fc.write('extern "C" {\n')
                fc.write(f'void context{self.idx}_head{h}(int8_t * x, int8_t * v, int8_t * a){{\n')
                fc.write(f'    context<{self.m}, {self.k}, {self.n}, {self.T//self.m}, {self.T//self.k}, {self.head_dim//self.n}, {self.shift_c[h]}>')
                fc.write('(x, v, a);\n')
                fc.write('}\n')
                fc.write('}')

        # Generate concat and output kernels
        if self.num_heads == 4:
            with open(f"iron_kernels/layer_{self.idx}_concat.cc", "w") as fc:
                fc.write('#include "iron_kernels.h"\n\n')

                fc.write('extern "C" {\n')
                fc.write(f'// Concat head 0 and head 1: ({self.T},{self.head_dim}) + ({self.T},{self.head_dim}) -> ({self.T},{self.head_dim*2})\n')
                fc.write(f'void concat{self.idx}_0(int8_t * sA, int8_t * sB, int8_t * sC){{\n')
                fc.write(f'    concat<{self.m}, {self.n}, {self.T//self.m}, {self.head_dim//self.n}>(sA, sB, sC);\n')
                fc.write('}\n\n')
                fc.write(f'// Concat head 2 and head 3: ({self.T},{self.head_dim}) + ({self.T},{self.head_dim}) -> ({self.T},{self.head_dim*2})\n')
                fc.write(f'void concat{self.idx}_1(int8_t * sA, int8_t * sB, int8_t * sC){{\n')
                fc.write(f'    concat<{self.m}, {self.n}, {self.T//self.m}, {self.head_dim//self.n}>(sA, sB, sC);\n')
                fc.write('}\n')
                fc.write('}')

            with open(f"iron_kernels/layer_{self.idx}_out.cc", "w") as fo:
                fo.write('#include "iron_kernels.h"\n\n')

                fo.write('extern "C" {\n')
                fo.write('#include <cstdint>\n')
                fo.write(f'int8_t k_p [{self.Wo_tiled.size}] = {{ ')
                fo.write(', '.join(str(int(x)) for x in self.Wo_tiled))
                fo.write(' };\n\n')
                fo.write(f'void out{self.idx}(int8_t * sA, int8_t * sB, int8_t * a){{\n')
                fo.write(f'    output<{self.m}, {self.k}, {self.n}, {self.T//self.m}, {self.d_model//self.k}, {self.d_model//self.n}, {self.shift_o}>')
                fo.write('(sA, sB, a, k_p);\n')
                fo.write('}\n')
                fo.write('}')

        elif self.num_heads == 1:
            with open(f"iron_kernels/layer_{self.idx}_out.cc", "w") as fo:
                fc.write('#include "iron_kernels.h"\n\n')

                fo.write('extern "C" {\n')
                fo.write('#include <cstdint>\n')
                fo.write(f'int8_t k_p [{self.Wo_tiled.size}] = {{ ')
                fo.write(', '.join(str(int(x)) for x in self.Wo_tiled))
                fo.write(' };\n\n')
                fo.write(f'void out{self.idx}(int8_t * x, int8_t * a){{\n')
                fo.write(f'    dense<{self.m}, {self.k}, {self.n}, {self.T//self.m}, {self.d_model//self.k}, {self.head_dim//self.n}, {self.shift_o}, false>')
                fo.write('(x, a, k_p);\n')
                fo.write('}\n')
                fo.write('}')

        self._generate_include_code()

    def _generate_include_code(self) -> None:
        """Append function declarations to include.h (private method)."""
        with open("aie/include.h", "a") as f:
            for h in range(self.num_heads):
                f.write(f'void q{self.idx}_head{h}(input_stream_int8 * __restrict, output_stream_int8 * __restrict);\n')
                f.write(f'void k{self.idx}_head{h}(input_stream_int8 * __restrict, output_stream_int8 * __restrict);\n')
                f.write(f'void v{self.idx}_head{h}(input_stream_int8 * __restrict, output_stream_int8 * __restrict);\n')
                f.write(f'void scores{self.idx}_head{h}(input_stream_int8 * __restrict, input_stream_int8 * __restrict, output_stream_int8 * __restrict);\n')
                f.write(f'void context{self.idx}_head{h}(input_stream_int8 * __restrict, input_stream_int8 * __restrict, output_stream_int8 * __restrict);\n')

            if self.num_heads == 4:
                f.write(f'void concat{self.idx}_0(input_stream_int8 * __restrict, input_stream_int8 * __restrict, output_stream_int8 * __restrict);\n')
                f.write(f'void concat{self.idx}_1(input_stream_int8 * __restrict, input_stream_int8 * __restrict, output_stream_int8 * __restrict);\n')
                f.write(f'void out{self.idx}(input_stream_int8 * __restrict, input_stream_int8 * __restrict, output_stream_int8 * __restrict);\n\n')
            else:
                f.write(f'void out{self.idx}(input_stream_int8 * __restrict, output_stream_int8 * __restrict);\n\n')

    def generate_graph_code(self, f, input_ports: List[str]) -> None:
        """
        Generate graph connectivity code for MHA.

        Args:
            f: File handle to write to
            input_ports: List with single input port name
        """
        self.validate_inputs(input_ports, expected_count=1)
        in_port = input_ports[0]

        for h in range(self.num_heads):
            base = h * 5  # Each head has 5 kernels: q, k, v, scores, context

            q_idx = base + 0
            k_idx = base + 1
            v_idx = base + 2
            scores_idx = base + 3
            context_idx = base + 4

            f.write(f"        {self.name}[{q_idx}] = kernel::create(::q{self.idx}_head{h});\n")
            f.write(f'        source({self.name}[{q_idx}]) = "layer_{self.idx}_q_head{h}.cc";\n')
            f.write(f'        runtime<ratio>({self.name}[{q_idx}]) = 1.0;\n')
            f.write(f"        connect<stream>({in_port}.out[0], {self.name}[{q_idx}].in[0]);\n\n")

            f.write(f"        {self.name}[{k_idx}] = kernel::create(::k{self.idx}_head{h});\n")
            f.write(f'        source({self.name}[{k_idx}]) = "layer_{self.idx}_k_head{h}.cc";\n')
            f.write(f'        runtime<ratio>({self.name}[{k_idx}]) = 1.0;\n')
            f.write(f"        connect<stream>({in_port}.out[0], {self.name}[{k_idx}].in[0]);\n\n")

            f.write(f"        {self.name}[{v_idx}] = kernel::create(::v{self.idx}_head{h});\n")
            f.write(f'        source({self.name}[{v_idx}]) = "layer_{self.idx}_v_head{h}.cc";\n')
            f.write(f'        runtime<ratio>({self.name}[{v_idx}]) = 1.0;\n')
            f.write(f"        connect<stream>({in_port}.out[0], {self.name}[{v_idx}].in[0]);\n\n")

            f.write(f"        {self.name}[{scores_idx}] = kernel::create(::scores{self.idx}_head{h});\n")
            f.write(f'        source({self.name}[{scores_idx}]) = "layer_{self.idx}_scores_head{h}.cc";\n')
            f.write(f'        runtime<ratio>({self.name}[{scores_idx}]) = 1.0;\n')
            f.write(f"        connect<stream> s{self.idx}_{h}_qk({self.name}[{q_idx}].out[0], {self.name}[{scores_idx}].in[0]);\n")
            f.write(f"        fifo_depth(s{self.idx}_{h}_qk) = {int(self.T*self.head_dim/4)};\n")
            f.write(f"        connect<stream>({self.name}[{k_idx}].out[0], {self.name}[{scores_idx}].in[1]);\n\n")

            f.write(f"        {self.name}[{context_idx}] = kernel::create(::context{self.idx}_head{h});\n")
            f.write(f'        source({self.name}[{context_idx}]) = "layer_{self.idx}_context_head{h}.cc";\n')
            f.write(f'        runtime<ratio>({self.name}[{context_idx}]) = 1.0;\n')
            f.write(f"        connect<stream> s{self.idx}_{h}_sv({self.name}[{scores_idx}].out[0], {self.name}[{context_idx}].in[0]);\n")
            f.write(f"        fifo_depth(s{self.idx}_{h}_sv) = {int(self.T*self.head_dim/4)};\n")
            f.write(f"        connect<stream>({self.name}[{v_idx}].out[0], {self.name}[{context_idx}].in[1]);\n\n")

        if self.num_heads == 4:
            concat_0_idx = self.num_heads * 5      # 20
            concat_1_idx = self.num_heads * 5 + 1  # 21
            out_idx = self.num_heads * 5 + 2       # 22

            head0_ctx = 4
            head1_ctx = 9
            head2_ctx = 14
            head3_ctx = 19

            # Create concat_0 kernel: head0 + head1
            f.write(f"        {self.name}[{concat_0_idx}] = kernel::create(::concat{self.idx}_0);\n")
            f.write(f'        source({self.name}[{concat_0_idx}]) = "layer_{self.idx}_concat.cc";\n')
            f.write(f'        runtime<ratio>({self.name}[{concat_0_idx}]) = 1.0;\n')
            f.write(f"        connect<stream>({self.name}[{head0_ctx}].out[0], {self.name}[{concat_0_idx}].in[0]);\n")
            f.write(f"        connect<stream>({self.name}[{head1_ctx}].out[0], {self.name}[{concat_0_idx}].in[1]);\n\n")

            # Create concat_1 kernel: head2 + head3
            f.write(f"        {self.name}[{concat_1_idx}] = kernel::create(::concat{self.idx}_1);\n")
            f.write(f'        source({self.name}[{concat_1_idx}]) = "layer_{self.idx}_concat.cc";\n')
            f.write(f'        runtime<ratio>({self.name}[{concat_1_idx}]) = 1.0;\n')
            f.write(f"        connect<stream>({self.name}[{head2_ctx}].out[0], {self.name}[{concat_1_idx}].in[0]);\n")
            f.write(f"        connect<stream>({self.name}[{head3_ctx}].out[0], {self.name}[{concat_1_idx}].in[1]);\n\n")

            # Create output projection kernel
            f.write(f"        {self.name}[{out_idx}] = kernel::create(::out{self.idx});\n")
            f.write(f'        source({self.name}[{out_idx}]) = "layer_{self.idx}_out.cc";\n')
            f.write(f'        runtime<ratio>({self.name}[{out_idx}]) = 1.0;\n')
            f.write(f"        connect<stream>({self.name}[{concat_0_idx}].out[0], {self.name}[{out_idx}].in[0]);\n")
            f.write(f"        connect<stream>({self.name}[{concat_1_idx}].out[0], {self.name}[{out_idx}].in[1]);\n\n")

        elif self.num_heads == 1:
            out_idx = 5
            context_idx = 4

            f.write(f"        {self.name}[{out_idx}] = kernel::create(::out{self.idx});\n")
            f.write(f'        source({self.name}[{out_idx}]) = "layer_{self.idx}_out.cc";\n')
            f.write(f'        runtime<ratio>({self.name}[{out_idx}]) = 1.0;\n')
            f.write(f'        connect<stream>({self.name}[{context_idx}].out[0], {self.name}[{out_idx}].in[0]);\n\n')

    def num_kernels(self) -> int:
        """Return number of AIE kernels for this MHA layer."""
        if self.num_heads == 4:
            return 23  # 4*5 + 2 concat + 1 output
        elif self.num_heads == 1:
            return 6   # 1*5 + 1 output
        else:
            raise ValueError(f"Unsupported num_heads: {self.num_heads}")

    def get_output_port(self, port_idx: int = 0) -> str:
        """Get output port name (the output projection kernel)."""
        if self.num_heads == 4:
            return f"{self.name}[22]"
        elif self.num_heads == 1:
            return f"{self.name}[5]"
        else:
            raise ValueError(f"Unsupported num_heads: {self.num_heads}")

    def __repr__(self) -> str:
        """String representation for debugging."""
        idx_str = f"idx={self.idx}" if self.idx is not None else "idx=unassigned"
        return (f"MHALayer({idx_str}, name='{self.name}', "
                f"num_heads={self.num_heads}, d_model={self.d_model}, "
                f"num_kernels={self.num_kernels()})")
